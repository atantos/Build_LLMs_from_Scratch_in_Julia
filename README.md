# Building LLMs from Scratch in Julia

This repository is dedicated to translating the Python code from the book **[Build a Large Language Model (from Scratch)](https://a.co/d/8m6dP1m)** by [Sebastian Raschka](https://github.com/rasbt) into Julia. The goal is to provide a Julia-based implementation of the concepts and techniques outlined in the book.

## üìö About the Book

[Sebastian Raschka](https://github.com/rasbt/rasbt)'s *Build a Large Language Model (from Scratch)* provides a hands-on approach to understanding the fundamental concepts behind large language models (LLMs), covering everything from tokenization to model training and inference. The book uses Python as the primary language, and this repository aims to make the same techniques accessible to Julia enthusiasts.

## üéØ Objectives

1. **Julia Translation**: Translate the Python code examples from the book into idiomatic Julia, ensuring clarity and performance.
2. **Julia Ecosystem**: Showcase how Julia's libraries and tools can be used to implement LLM components, such as tokenization, embedding layers, and model architectures.
3. **Learning Resource**: Provide Julia users with a resource to learn about LLM development in their preferred language.

## ‚úçÔ∏è Medium Articles

The code in this repository is explained through a series of Medium articles. Each new commit in this repository will be accompanied by a new Medium article. Below are the Medium articles published so far: 

### Chapter 3

Section 3.3.2: [Computing Attention Weights for All Input Tokens](https://medium.com/@atantos/computing-attention-weights-for-all-input-tokens-fab6d912aeae). Here is the [friend link](https://medium.com/@atantos/computing-attention-weights-for-all-input-tokens-fab6d912aeae?sk=2e217edafbceff61d2f9aac8e8467f2b) for non-Medium members.

Section 3.3.1: [Implementing a Simple Self-Attention Mechanism with Python and Julia](https://medium.com/juliazoid/implementing-attention-mechanisms-with-python-and-julia-f453c2728364). Here is the [friend link](https://medium.com/juliazoid/implementing-attention-mechanisms-with-python-and-julia-f453c2728364?sk=5d1235e64ad873b08557554c68d0713c) for non-Medium members.

### Chapter 2

Section 2.8: [How to Add Positional Encodings to Token Embeddings in Python and Julia for LLM Training](https://medium.com/juliazoid/how-to-add-positional-encodings-to-token-embeddings-in-python-and-julia-for-llm-training-5b8ccf17f1e3). Here is the [friend link](https://medium.com/juliazoid/how-to-add-positional-encodings-to-token-embeddings-in-python-and-julia-for-llm-training-5b8ccf17f1e3?sk=7d0556b7d299cb4e3ac93d388ccaf2f6) for non-Medium members.

Section 2.7: [From Token IDs to Embeddings: LLM Training with Python and Julia](https://medium.com/@atantos/from-token-ids-to-embeddings-llm-training-with-python-and-julia-18b39641b420). Here is the [friend link](https://medium.com/@atantos/from-token-ids-to-embeddings-llm-training-with-python-and-julia-18b39641b420?sk=7cd1e15795f14a5ea28a6c66933b5a40) for non-Medium members. 

Section 2.6b: [Creating a PyTorch-Style DataLoader in Julia for LLM Training Batches](https://medium.com/juliazoid/creating-a-pytorch-style-dataloader-in-julia-for-llm-training-batches-072e0c4492e1). Here is the [friend link](https://medium.com/juliazoid/creating-a-pytorch-style-dataloader-in-julia-for-llm-training-batches-072e0c4492e1?sk=ea49248a7b31fbae038a1fc0ae32a2ea) for non-Medium members.

Section 2.6a: [Preparing a PyTorch-Style Dataset in Julia for LLM Training](https://medium.com/@atantos/preparing-a-pytorch-style-dataset-in-julia-for-llm-training-f415202cedf9). Here is the [friend link](https://medium.com/@atantos/preparing-a-pytorch-style-dataset-in-julia-for-llm-training-f415202cedf9?sk=5f68fd77b788e808c5d3173902d6ab09) for non-Medium members.

Section 2.5: [Byte Pair Encoding (BPE) in Action: Julia and Python Side-by-Side](https://medium.com/@atantos/byte-pair-encoding-bpe-in-action-julia-and-python-side-by-side-7471b87e19ca). Here is the [friend link](https://medium.com/@atantos/byte-pair-encoding-bpe-in-action-julia-and-python-side-by-side-7471b87e19ca?sk=2ccce14a6ccd24c4c3d89fd7078fa4de) for non-Medium members.

Section 2.3: [From Classes to Structs: Translating a Python Tokenizer into Julia‚Äôs Functional Style](https://medium.com/@atantos/from-classes-to-structs-translating-a-python-tokenizer-into-julias-functional-style-204a85f17afb). Here is the [friend link](https://medium.com/@atantos/from-classes-to-structs-translating-a-python-tokenizer-into-julias-functional-style-204a85f17afb?sk=c2226a40ef82586668143437fab8d29c) for non-Medium members.

Section 2.3: [Creating a Vocabulary Dictionary for English Words in Julia](https://medium.com/@atantos/creating-a-vocabulary-dictionary-for-english-words-in-julia-0ae71523f47d). Here is the [friend link](https://medium.com/@atantos/creating-a-vocabulary-dictionary-for-english-words-in-julia-0ae71523f47d?sk=91f286aecc85e89d05aa800f50f049a7) for non-Medium members.

Section 


## ü§ù Contributing
Contributions are welcome! If you find a bug, have a suggestion, or want to improve the code, feel free to open an issue or submit a pull request.

## üì¨ Feedback
If you have questions or feedback, feel free to open an issue or reach out to me on Twitter.

## üôè Acknowledgments
Sebastian Raschka for the incredible book and the Python code examples that inspired this repo.

Happy coding! üöÄ
